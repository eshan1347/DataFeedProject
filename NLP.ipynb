{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ro_go1bynTyG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import praw\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "cv = CountVectorizer(ngram_range=(1,2))\n",
        "mn  = MultinomialNB()\n",
        "port = PorterStemmer()"
      ],
      "metadata": {
        "id": "avwa6gjf8mgX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanText(text):\n",
        "  t0 = text.lower()\n",
        "  t1 = tokenizer.tokenize(t0)\n",
        "  t2 = [t for t in t1 if t not in stopwords.words('english')]\n",
        "  t3 = [port.stem(t) for t in t2]\n",
        "  t4 = \" \".join(t3)\n",
        "  return t4\n",
        "def redScrape(num):\n",
        "    user_agent = \"Scraper 1.0\"\n",
        "    reddit = praw.Reddit(\n",
        "        client_id = \"CCR9qratPsoFxW6lUtLsiA\",\n",
        "        client_secret = \"DrlkhN7yc9u0tgj1caxThBcNzjB-sA\",\n",
        "        user_agent=user_agent\n",
        "    )\n",
        "    text = []\n",
        "    headlines = set()\n",
        "    posts = reddit.subreddit('politics').hot(limit=num)\n",
        "    for post in posts:\n",
        "        # print(post.author)\n",
        "        # print(post.title)\n",
        "        text.append(post.title)\n",
        "        # print(post.selftext)\n",
        "    return text"
      ],
      "metadata": {
        "id": "VxN49ugY9fIj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentimentAnalyse(path,trainingLimits,postLimit):\n",
        "  data = pd.read_csv(path)\n",
        "  data.columns = [\"Num\",\"Topic\",\"Class\",\"Sentence\"]\n",
        "  data = data.dropna()\n",
        "  X = data.iloc[:trainingLimits,3]\n",
        "  Y = data.iloc[:trainingLimits,2]  \n",
        "  le.fit(Y)\n",
        "  Y = le.transform(Y)\n",
        "  X_clean = [cleanText(t) for t in X]\n",
        "  X_clean = pd.DataFrame(X_clean)\n",
        "  #Irrelevant - 0\n",
        "  #Negative - 1\n",
        "  #Neutral - 2\n",
        "  #Positive - 3\n",
        "  topic = np.full(X.shape[0],-1)\n",
        "  X_clean['topic'] = topic\n",
        "  for i in range(X_clean.shape[0]):\n",
        "    res = X_clean.iloc[i,0].split(\" \")\n",
        "    for r in res:\n",
        "      if r == \"come\":\n",
        "        X_clean.iloc[i,1] = 0\n",
        "      if r == \"Crime\":\n",
        "        X_clean.iloc[i,1] = 1\n",
        "      if r == \"Transport\":\n",
        "        X_clean.iloc[i,1] = 2\n",
        "      if r == \"Health\":\n",
        "        X_clean.iloc[i,1] = 3\n",
        "      if r == \"Hygiene\":\n",
        "        X_clean.iloc[i,1] = 4\n",
        "  X_vec = cv.fit_transform(X_clean[0]).toarray()\n",
        "  mn.fit(X_vec,Y)\n",
        "  text = redScrape(postLimit)\n",
        "  text_clean = [cleanText(t) for t in X]\n",
        "  text_clean = pd.DataFrame(text_clean)\n",
        "  topic = np.full(text_clean.shape[0],-1)\n",
        "  text_clean['topic'] = topic\n",
        "  for i in range(text_clean.shape[0]):\n",
        "    res = text_clean.iloc[i,0].split(\" \")\n",
        "    for r in res:\n",
        "      if r == \"Education\":\n",
        "        text_clean.iloc[i,1] = 0\n",
        "      if r == \"Crime\":\n",
        "        text_clean.iloc[i,1] = 1\n",
        "      if r == \"Transport\":\n",
        "        text_clean.iloc[i,1] = 2\n",
        "      if r == \"Health\":\n",
        "        text_clean.iloc[i,1] = 3\n",
        "      if r == \"Hygiene\":\n",
        "        text_clean.iloc[i,1] = 4\n",
        "  text_vec = cv.fit_transform(text_clean[0]).toarray()\n",
        "  Y_pred = mn.predict(text_vec)\n",
        "  op = np.full((Y_pred.shape[0],5),0)\n",
        "  for i in range(Y_pred.shape[0]):\n",
        "    if X_clean.iloc[i,1] == 0:\n",
        "      op[i,0] = Y_pred[i]\n",
        "    if X_clean.iloc[i,1] == 1:\n",
        "      op[i,1] = Y_pred[i]\n",
        "    if X_clean.iloc[i,1] == 2:\n",
        "      op[i,2] = Y_pred[i]\n",
        "    if X_clean.iloc[i,1] == 3:\n",
        "      op[i,3] = Y_pred[i]\n",
        "    if X_clean.iloc[i,1] == 4:\n",
        "      op[i,4] = Y_pred[i]\n",
        "  return op"
      ],
      "metadata": {
        "id": "zBOnyylnKgzr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentimentAnalyse(\"/home/eshan/Eshan/Code Linux/twitterSentAnalysis/twitter_training.csv\",100,100)"
      ],
      "metadata": {
        "id": "VQ9p9d5OJ0q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CvWyLOImNtbC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}